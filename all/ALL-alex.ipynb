{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>ENDLESS<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#am-(done)\" data-toc-modified-id=\"am-(done)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>am (done)</a></span><ul class=\"toc-item\"><li><span><a href=\"#construct-the-model\" data-toc-modified-id=\"construct-the-model-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>construct the model</a></span></li><li><span><a href=\"#get-train-params\" data-toc-modified-id=\"get-train-params-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>get train params</a></span></li><li><span><a href=\"#tune-hyper-params\" data-toc-modified-id=\"tune-hyper-params-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>tune hyper params</a></span></li><li><span><a href=\"#prepare-data-and-metrics\" data-toc-modified-id=\"prepare-data-and-metrics-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>prepare data and metrics</a></span></li><li><span><a href=\"#optimizer\" data-toc-modified-id=\"optimizer-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>optimizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#sgd\" data-toc-modified-id=\"sgd-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>sgd</a></span></li><li><span><a href=\"#adam\" data-toc-modified-id=\"adam-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>adam</a></span></li><li><span><a href=\"#adam0\" data-toc-modified-id=\"adam0-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>adam0</a></span></li><li><span><a href=\"#adam0_1\" data-toc-modified-id=\"adam0_1-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>adam0_1</a></span></li></ul></li><li><span><a href=\"#run\" data-toc-modified-id=\"run-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>run</a></span></li><li><span><a href=\"#draw-pic\" data-toc-modified-id=\"draw-pic-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>draw pic</a></span><ul class=\"toc-item\"><li><span><a href=\"#loss\" data-toc-modified-id=\"loss-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>loss</a></span></li><li><span><a href=\"#train-accu\" data-toc-modified-id=\"train-accu-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>train accu</a></span></li><li><span><a href=\"#test-accu\" data-toc-modified-id=\"test-accu-1.7.3\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;</span>test accu</a></span></li></ul></li></ul></li><li><span><a href=\"#ao(done)\" data-toc-modified-id=\"ao(done)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>ao(done)</a></span><ul class=\"toc-item\"><li><span><a href=\"#construct-the-model\" data-toc-modified-id=\"construct-the-model-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>construct the model</a></span></li><li><span><a href=\"#get-train-params\" data-toc-modified-id=\"get-train-params-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>get train params</a></span></li><li><span><a href=\"#tune-hyper-params\" data-toc-modified-id=\"tune-hyper-params-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>tune hyper params</a></span></li><li><span><a href=\"#prepare-data-and-metrics\" data-toc-modified-id=\"prepare-data-and-metrics-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>prepare data and metrics</a></span></li><li><span><a href=\"#optimizers\" data-toc-modified-id=\"optimizers-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>optimizers</a></span><ul class=\"toc-item\"><li><span><a href=\"#sgd\" data-toc-modified-id=\"sgd-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>sgd</a></span></li><li><span><a href=\"#adam\" data-toc-modified-id=\"adam-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>adam</a></span></li><li><span><a href=\"#adam0\" data-toc-modified-id=\"adam0-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>adam0</a></span></li><li><span><a href=\"#adam0_1\" data-toc-modified-id=\"adam0_1-2.5.4\"><span class=\"toc-item-num\">2.5.4&nbsp;&nbsp;</span>adam0_1</a></span></li></ul></li><li><span><a href=\"#run\" data-toc-modified-id=\"run-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>run</a></span></li><li><span><a href=\"#draw-pic\" data-toc-modified-id=\"draw-pic-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>draw pic</a></span><ul class=\"toc-item\"><li><span><a href=\"#loss\" data-toc-modified-id=\"loss-2.7.1\"><span class=\"toc-item-num\">2.7.1&nbsp;&nbsp;</span>loss</a></span></li><li><span><a href=\"#train-accu\" data-toc-modified-id=\"train-accu-2.7.2\"><span class=\"toc-item-num\">2.7.2&nbsp;&nbsp;</span>train accu</a></span></li><li><span><a href=\"#test-accu\" data-toc-modified-id=\"test-accu-2.7.3\"><span class=\"toc-item-num\">2.7.3&nbsp;&nbsp;</span>test accu</a></span></li></ul></li></ul></li><li><span><a href=\"#af\" data-toc-modified-id=\"af-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>af</a></span><ul class=\"toc-item\"><li><span><a href=\"#construct-the-model\" data-toc-modified-id=\"construct-the-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>construct the model</a></span></li><li><span><a href=\"#get-train-params\" data-toc-modified-id=\"get-train-params-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>get train params</a></span></li><li><span><a href=\"#tune-hyper-params\" data-toc-modified-id=\"tune-hyper-params-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>tune hyper params</a></span></li><li><span><a href=\"#prepare-data-and-metrics\" data-toc-modified-id=\"prepare-data-and-metrics-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>prepare data and metrics</a></span></li><li><span><a href=\"#optimizer\" data-toc-modified-id=\"optimizer-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>optimizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#sgd\" data-toc-modified-id=\"sgd-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>sgd</a></span></li><li><span><a href=\"#adam\" data-toc-modified-id=\"adam-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>adam</a></span></li><li><span><a href=\"#adam0\" data-toc-modified-id=\"adam0-3.5.3\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>adam0</a></span></li><li><span><a href=\"#adam0_1\" data-toc-modified-id=\"adam0_1-3.5.4\"><span class=\"toc-item-num\">3.5.4&nbsp;&nbsp;</span>adam0_1</a></span></li></ul></li><li><span><a href=\"#run\" data-toc-modified-id=\"run-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>run</a></span></li><li><span><a href=\"#draw-pic\" data-toc-modified-id=\"draw-pic-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>draw pic</a></span><ul class=\"toc-item\"><li><span><a href=\"#loss\" data-toc-modified-id=\"loss-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>loss</a></span></li><li><span><a href=\"#train-accu\" data-toc-modified-id=\"train-accu-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>train accu</a></span></li><li><span><a href=\"#test-accu\" data-toc-modified-id=\"test-accu-3.7.3\"><span class=\"toc-item-num\">3.7.3&nbsp;&nbsp;</span>test accu</a></span></li></ul></li></ul></li><li><span><a href=\"#ac\" data-toc-modified-id=\"ac-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>ac</a></span><ul class=\"toc-item\"><li><span><a href=\"#construct-the-model\" data-toc-modified-id=\"construct-the-model-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>construct the model</a></span></li><li><span><a href=\"#get-train-params\" data-toc-modified-id=\"get-train-params-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>get train params</a></span></li><li><span><a href=\"#tune-hyper-params\" data-toc-modified-id=\"tune-hyper-params-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>tune hyper params</a></span></li><li><span><a href=\"#prepare-data-and-metrics\" data-toc-modified-id=\"prepare-data-and-metrics-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>prepare data and metrics</a></span></li><li><span><a href=\"#optimizer\" data-toc-modified-id=\"optimizer-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>optimizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#sgd\" data-toc-modified-id=\"sgd-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>sgd</a></span></li><li><span><a href=\"#adam\" data-toc-modified-id=\"adam-4.5.2\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>adam</a></span></li><li><span><a href=\"#adam0\" data-toc-modified-id=\"adam0-4.5.3\"><span class=\"toc-item-num\">4.5.3&nbsp;&nbsp;</span>adam0</a></span></li><li><span><a href=\"#adam0_1\" data-toc-modified-id=\"adam0_1-4.5.4\"><span class=\"toc-item-num\">4.5.4&nbsp;&nbsp;</span>adam0_1</a></span></li></ul></li><li><span><a href=\"#run\" data-toc-modified-id=\"run-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>run</a></span></li><li><span><a href=\"#draw-pic\" data-toc-modified-id=\"draw-pic-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>draw pic</a></span><ul class=\"toc-item\"><li><span><a href=\"#loss\" data-toc-modified-id=\"loss-4.7.1\"><span class=\"toc-item-num\">4.7.1&nbsp;&nbsp;</span>loss</a></span></li><li><span><a href=\"#trian-accu\" data-toc-modified-id=\"trian-accu-4.7.2\"><span class=\"toc-item-num\">4.7.2&nbsp;&nbsp;</span>trian accu</a></span></li><li><span><a href=\"#test-accu\" data-toc-modified-id=\"test-accu-4.7.3\"><span class=\"toc-item-num\">4.7.3&nbsp;&nbsp;</span>test accu</a></span></li></ul></li></ul></li><li><span><a href=\"#at（coming-soon。。。）\" data-toc-modified-id=\"at（coming-soon。。。）-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>at（coming soon。。。）</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install tflearn\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "#@title \n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
    "\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n",
    "# 指定当前的工作文件夹\n",
    "import os\n",
    "# 此处为google drive中的文件路径,drive为之前指定的工作根目录，要加上\n",
    "os.chdir(\"drive/deep_learning\") \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T15:04:51.636763Z",
     "start_time": "2018-10-16T15:04:51.632774Z"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## am (done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T10:41:03.135599Z",
     "start_time": "2018-10-22T10:41:01.374237Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tflearn\\initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#################################################################\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/',one_hot=True)\n",
    "\n",
    "x=tf.placeholder(tf.float32,[None,784],name='x')\n",
    "y_=tf.placeholder(tf.float32,[None,10],name='y_');\n",
    "\n",
    "x_image=tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "\n",
    "#################################################################\n",
    "keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "# Building 'AlexNet'\n",
    "# network = input_data(shape=[None, 224, 224, 3])\n",
    "network = conv_2d(x_image, 96, 11, strides=4, activation='relu')\n",
    "network = max_pool_2d(network, 2, strides=2)\n",
    "network = local_response_normalization(network)\n",
    "network = conv_2d(network, 256, 5, activation='relu')\n",
    "network = max_pool_2d(network, 2, strides=2)\n",
    "network = local_response_normalization(network)\n",
    "network = conv_2d(network, 384, 3, activation='relu')\n",
    "network = conv_2d(network, 384, 3, activation='relu')\n",
    "network = conv_2d(network, 256, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2, strides=2)\n",
    "network = local_response_normalization(network)\n",
    "network = fully_connected(network, 4096, activation='tanh')\n",
    "network = dropout(network, keep_prob)\n",
    "network = fully_connected(network, 4096, activation='tanh')\n",
    "network = dropout(network, keep_prob)\n",
    "network = fully_connected(network, 10, activation='softmax')\n",
    "\n",
    "#################################################################\n",
    "y=network \n",
    "train_params = tf.trainable_variables()\n",
    "for i in range(len(train_params)):\n",
    "    print(train_params[i].dtype)\n",
    "#################################################################\n",
    "\n",
    "# network = regression(network, optimizer='momentum',\n",
    "#                      loss='categorical_crossentropy',\n",
    "#                      learning_rate=0.001)\n",
    "\n",
    "# # Training\n",
    "# model = tflearn.DNN(network, checkpoint_path='model_alexnet',\n",
    "#                     max_checkpoints=1, tensorboard_verbose=2)\n",
    "# model.fit(X, Y, n_epoch=1000, validation_set=0.1, shuffle=True,\n",
    "#           show_metric=True, batch_size=64, snapshot_step=200,\n",
    "#           snapshot_epoch=False, run_id='alexnet_oxflowers17')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get train params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T10:41:05.111084Z",
     "start_time": "2018-10-22T10:41:05.106098Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var=[]\n",
    "for i in range(len(train_params)):\n",
    "    var.append(train_params[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tune hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T10:41:06.866346Z",
     "start_time": "2018-10-22T10:41:06.861358Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lr=1e-3           #1e-4 instead\n",
    "BATCH_SIZE = 128   #16,32,64,128\n",
    "epoch = 500       #corrected, same as epcho\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T10:41:08.514511Z",
     "start_time": "2018-10-22T10:41:08.483066Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y+1e-8), reduction_indices = [1]))\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "losses    = {'sgd':[],'adam':[],'adam0':[],'adam0_1':[]} \n",
    "test_accu = {'sgd':[],'adam':[],'adam0':[],'adam0_1':[]}\n",
    "train_accu= {'sgd':[],'adam':[],'adam0':[],'adam0_1':[]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T10:41:11.751898Z",
     "start_time": "2018-10-22T10:41:11.707018Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd():\n",
    "    def mycapper0(gradient):\n",
    "        return gradient \n",
    "    opt =tf.train.GradientDescentOptimizer(learning_rate= lr)\n",
    "    grads_and_vars = opt.compute_gradients(cross_entropy,var,gate_gradients=opt.GATE_GRAPH)\n",
    "    capped_grads_and_vars = [(mycapper0(gv[0]), gv[1]) for gv in grads_and_vars]\n",
    "    train_step=opt.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    losses['sgd'].clear()\n",
    "    test_accu['sgd'].clear()\n",
    "    train_accu['sgd'].clear()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            batch=mnist.train.next_batch(BATCH_SIZE)\n",
    "            train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            losses['sgd'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            train_accu['sgd'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x:mnist.test.images[:2000],y_:mnist.test.labels[:2000],keep_prob:1.})\n",
    "            test_accu['sgd'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T10:41:14.557776Z",
     "start_time": "2018-10-22T10:41:14.529850Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adam():\n",
    "    train_step=tf.train.AdamOptimizer(learning_rate=lr).minimize(cross_entropy)\n",
    "    \n",
    "    losses['adam'].clear()\n",
    "    test_accu['adam'].clear()\n",
    "    train_accu['adam'].clear()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            batch=mnist.train.next_batch(BATCH_SIZE)\n",
    "            train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            losses['adam'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            train_accu['adam'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x:mnist.test.images[:2000],y_:mnist.test.labels[:2000],keep_prob:1.})\n",
    "            test_accu['adam'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adam0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T10:41:16.781475Z",
     "start_time": "2018-10-22T10:41:16.749562Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam0():\n",
    "    def mycapper4(grads_and_vars):\n",
    "        beta_1=0.9\n",
    "        beta_2=0.9\n",
    "        for i in range(len(grads_and_vars)):\n",
    "            capped_grads_and_vars_momentum_1[i]=(beta_1*capped_grads_and_vars[i][0]+(1-beta_1)*tf.sign(grads_and_vars[i][0]),grads_and_vars[i][1])\n",
    "            capped_grads_and_vars_momentum_2[i]=(beta_2*capped_grads_and_vars[i][0]+(1-beta_2),grads_and_vars[i][1])\n",
    "            capped_grads_and_vars[i]=(tf.sign(capped_grads_and_vars_momentum_1[i][0])/(tf.sqrt(capped_grads_and_vars_momentum_2[i][0])+1e-3),capped_grads_and_vars_momentum_1[i][1])\n",
    "        return capped_grads_and_vars   \n",
    "        \n",
    "    opt =tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    grads_and_vars = opt.compute_gradients(cross_entropy,var,gate_gradients=opt.GATE_GRAPH )\n",
    "    capped_grads_and_vars_momentum_1=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars_momentum_2=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=mycapper4(grads_and_vars)  \n",
    "    train_step=opt.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    losses['adam0'].clear()\n",
    "    test_accu['adam0'].clear()\n",
    "    train_accu['adam0'].clear()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            batch=mnist.train.next_batch(BATCH_SIZE)\n",
    "            train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            losses['adam0'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            train_accu['adam0'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x:mnist.test.images[:2000],y_:mnist.test.labels[:2000],keep_prob:1.})\n",
    "            test_accu['adam0'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adam0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T10:41:18.678307Z",
     "start_time": "2018-10-22T10:41:18.648388Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam0_1():\n",
    "    def mycapper4(grads_and_vars):\n",
    "        beta_1=0.9\n",
    "        beta_2=0.9\n",
    "        for i in range(len(grads_and_vars)):\n",
    "            x=tf.sign(grads_and_vars[i][0])\n",
    "            capped_grads_and_vars_momentum_1[i]=(beta_1*capped_grads_and_vars_momentum_1[i][0]+(1-beta_1)*x,grads_and_vars[i][1])\n",
    "            capped_grads_and_vars_momentum_2[i]=(beta_2*capped_grads_and_vars_momentum_2[i][0]+(1-beta_2)*x*x,grads_and_vars[i][1])\n",
    "            capped_grads_and_vars[i]=(tf.sign(capped_grads_and_vars_momentum_1[i][0])/(tf.sqrt(capped_grads_and_vars_momentum_2[i][0])+1e-7),capped_grads_and_vars_momentum_1[i][1])\n",
    "        return capped_grads_and_vars \n",
    "        \n",
    "    opt =tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    grads_and_vars = opt.compute_gradients(cross_entropy,var,gate_gradients=opt.GATE_GRAPH )\n",
    "    capped_grads_and_vars_momentum_1=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars_momentum_2=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=mycapper4(grads_and_vars)  \n",
    "    train_step=opt.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    losses['adam0_1'].clear()\n",
    "    test_accu['adam0_1'].clear()\n",
    "    train_accu['adam0_1'].clear()\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            batch=mnist.train.next_batch(BATCH_SIZE)\n",
    "            train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            losses['adam0_1'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            train_accu['adam0_1'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x:mnist.test.images[:2000],y_:mnist.test.labels[:2000],keep_prob:1.})\n",
    "            test_accu['adam0_1'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T10:41:46.712557Z",
     "start_time": "2018-10-22T10:41:23.505980Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 step\n",
      "train_loss_now : 5.217682\n",
      "train_accu_now : 0.1484375\n",
      "test_accu_now : 0.1025\n",
      "1 step\n",
      "train_loss_now : 2.3179789\n",
      "train_accu_now : 0.078125\n",
      "test_accu_now : 0.097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-60f38bd3d009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0madam0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0madam0_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-d8f3c229be5c>\u001b[0m in \u001b[0;36madam\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtrain_accu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_accu_now\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[1;31m### test_accu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtest_accu_now\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mtest_accu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_accu_now\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[1;31m### print things\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m     \"\"\"\n\u001b[0;32m--> 711\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5153\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   5154\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5155\u001b[0;31m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   5157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adam()\n",
    "adam0()\n",
    "adam0_1()\n",
    "sgd()\n",
    "\n",
    "model_name='alex'\n",
    "data_name='mnist'\n",
    "\n",
    "def save_me():\n",
    "    loss_name=\"npy/losses batch\"+str(BATCH_SIZE)+'lr'+str(lr)+'iter'+str(epoch)+str(model_name)+str(data_name)+\".npy\"\n",
    "    np.save(loss_name,losses)\n",
    "    \n",
    "    trainaccu_name=\"npy/trainaccu batch\"+str(BATCH_SIZE)+'lr'+str(lr)+'iter'+str(epoch)+str(model_name)+str(data_name)+\".npy\"\n",
    "    np.save(trainaccu_name,train_accu)\n",
    "    \n",
    "    testaccu_name=\"npy/testaccu batch\"+str(BATCH_SIZE)+'lr'+str(lr)+'iter'+str(epoch)+str(model_name)+str(data_name)+\".npy\"\n",
    "    np.save(testaccu_name,test_accu)\n",
    "    \n",
    "save_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(100,5))\n",
    "plt.plot(losses['sgd'], label='SGD')\n",
    "plt.plot(losses['adam'], label='ADAM')\n",
    "plt.plot(losses['adam0'], label='ADAM0')\n",
    "plt.plot(losses['adam0_1'], label='ADAM0_1')\n",
    "\n",
    "plt.title('learning rate = '+str(lr))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,5))\n",
    "plt.plot(train_accu['sgd'], label='SGD')\n",
    "plt.plot(train_accu['adam'], label='ADAM')\n",
    "plt.plot(train_accu['adam0'], label='ADAM0')\n",
    "plt.plot(train_accu['adam0_1'], label='ADAM0_1')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('learning rate = '+str(lr))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"train accuracy\")\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,5))\n",
    "plt.plot(test_accu['sgd'], label='SGD')\n",
    "plt.plot(test_accu['adam'], label='ADAM')\n",
    "plt.plot(test_accu['adam0'], label='ADAM0')\n",
    "plt.plot(test_accu['adam0_1'], label='ADAM0_1')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('learning rate = '+str(lr))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"test accuracy\")\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ao(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T14:37:09.784843Z",
     "start_time": "2018-10-22T14:36:59.006666Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#################################################################\n",
    "X, Y = oxflower17.load_data(one_hot=True, resize_pics=(224, 224))\n",
    "\n",
    "x=tf.placeholder(tf.float32,[None,224,224,3],name='x')\n",
    "y_=tf.placeholder(tf.float32,[None,17],name='y_');\n",
    "x_image=x\n",
    "#################################################################\n",
    "keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "# Building 'AlexNet'\n",
    "# network = input_data(shape=[None, 224, 224, 3])\n",
    "network = conv_2d(x_image, 96, 11, strides=4, activation='relu')\n",
    "network = max_pool_2d(network, 3, strides=2)\n",
    "network = local_response_normalization(network)\n",
    "network = conv_2d(network, 256, 5, activation='relu')\n",
    "network = max_pool_2d(network, 3, strides=2)\n",
    "network = local_response_normalization(network)\n",
    "network = conv_2d(network, 384, 3, activation='relu')\n",
    "network = conv_2d(network, 384, 3, activation='relu')\n",
    "network = conv_2d(network, 256, 3, activation='relu')\n",
    "network = max_pool_2d(network, 3, strides=2)\n",
    "network = local_response_normalization(network)\n",
    "network = fully_connected(network, 4096, activation='tanh')\n",
    "network = dropout(network, keep_prob)\n",
    "network = fully_connected(network, 4096, activation='tanh')\n",
    "network = dropout(network, keep_prob)\n",
    "network = fully_connected(network, 17, activation='softmax')\n",
    "\n",
    "#################################################################\n",
    "y=network \n",
    "train_params = tf.trainable_variables()\n",
    "for i in range(len(train_params)):\n",
    "    print((train_params[i].dtype))\n",
    "#################################################################\n",
    "\n",
    "# network = regression(network, optimizer='momentum',\n",
    "#                      loss='categorical_crossentropy',\n",
    "#                      learning_rate=0.001)\n",
    "\n",
    "# # Training\n",
    "# model = tflearn.DNN(network, checkpoint_path='model_alexnet',\n",
    "#                     max_checkpoints=1, tensorboard_verbose=2)\n",
    "# model.fit(X, Y, n_epoch=1000, validation_set=0.1, shuffle=True,\n",
    "#           show_metric=True, batch_size=64, snapshot_step=200,\n",
    "#           snapshot_epoch=False, run_id='alexnet_oxflowers17')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get train params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T09:52:56.464510Z",
     "start_time": "2018-10-19T09:52:56.460522Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var=[]\n",
    "for i in range(len(train_params)):\n",
    "    var.append(train_params[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tune hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T09:52:59.023907Z",
     "start_time": "2018-10-19T09:52:59.017893Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size=1000\n",
    "test_size=200     #can't be too big \n",
    "lr=1e-3           #1e-4 instead\n",
    "BATCH_SIZE = 16   #16,32,64,128\n",
    "epoch = 500       #corrected, same as epcho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T09:53:02.154060Z",
     "start_time": "2018-10-19T09:53:02.133117Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "x_train=X[:train_size]\n",
    "y_train=Y[:train_size]\n",
    "x_test=X[train_size:]\n",
    "y_test=Y[train_size:]\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y+1e-8), reduction_indices = [1]))\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "losses    = {'sgd':[],'adam':[],'adam0':[],'adam0_1':[]} \n",
    "test_accu = {'sgd':[],'adam':[],'adam0':[],'adam0_1':[]}\n",
    "train_accu= {'sgd':[],'adam':[],'adam0':[],'adam0_1':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T09:53:04.704279Z",
     "start_time": "2018-10-19T09:53:04.670400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def sgd():\n",
    "    def mycapper0(gradient):\n",
    "        return gradient \n",
    "    opt =tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    grads_and_vars = opt.compute_gradients(cross_entropy,var,gate_gradients=opt.GATE_GRAPH)\n",
    "    capped_grads_and_vars = [(mycapper0(gv[0]), gv[1]) for gv in grads_and_vars]\n",
    "    train_step=opt.apply_gradients(capped_grads_and_vars)\n",
    "  \n",
    "\n",
    "    losses['sgd'].clear()\n",
    "    test_accu['sgd'].clear()\n",
    "    train_accu['sgd'].clear()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            ### batch run\n",
    "            start = i * BATCH_SIZE % train_size\n",
    "            train_step.run(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:0.5})\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:1})\n",
    "            losses['sgd'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                    y_: y_train[start: start + BATCH_SIZE],keep_prob:1})\n",
    "            train_accu['sgd'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x:x_test[0:test_size],y_:y_test[0:test_size],keep_prob:1})\n",
    "            test_accu['sgd'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T09:53:07.488959Z",
     "start_time": "2018-10-19T09:53:07.453059Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def adam():\n",
    "    train_step=tf.train.AdamOptimizer(learning_rate=lr).minimize(cross_entropy)\n",
    "\n",
    "    losses['adam'].clear()\n",
    "    test_accu['adam'].clear()\n",
    "    train_accu['adam'].clear()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            ### batch run\n",
    "            start = i * BATCH_SIZE % train_size\n",
    "            train_step.run(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:0.5})\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:1})\n",
    "            losses['adam'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                    y_: y_train[start: start + BATCH_SIZE],keep_prob:1})\n",
    "            train_accu['adam'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x:x_test[0:test_size],y_:y_test[0:test_size],keep_prob:1})\n",
    "            test_accu['adam'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adam0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T09:53:09.785434Z",
     "start_time": "2018-10-19T09:53:09.731598Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam0():\n",
    "    def mycapper4(grads_and_vars):\n",
    "        beta_1=0.9\n",
    "        beta_2=0.9\n",
    "        for i in range(len(grads_and_vars)):\n",
    "            capped_grads_and_vars_momentum_1[i]=(beta_1*capped_grads_and_vars[i][0]+(1-beta_1)*tf.sign(grads_and_vars[i][0]),grads_and_vars[i][1])\n",
    "            capped_grads_and_vars_momentum_2[i]=(beta_2*capped_grads_and_vars[i][0]+(1-beta_2),grads_and_vars[i][1])\n",
    "            capped_grads_and_vars[i]=(tf.sign(capped_grads_and_vars_momentum_1[i][0])/(tf.sqrt(capped_grads_and_vars_momentum_2[i][0])+1e-3),capped_grads_and_vars_momentum_1[i][1])\n",
    "        return capped_grads_and_vars   \n",
    "        \n",
    "    opt =tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    grads_and_vars = opt.compute_gradients(cross_entropy,var,gate_gradients=opt.GATE_GRAPH )\n",
    "    capped_grads_and_vars_momentum_1=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars_momentum_2=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=mycapper4(grads_and_vars)  \n",
    "    train_step=opt.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    losses['adam0'].clear()\n",
    "    test_accu['adam0'].clear()\n",
    "    train_accu['adam0'].clear()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            ### batch run\n",
    "            start = i * BATCH_SIZE % train_size\n",
    "            train_step.run(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:0.5})\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:1})\n",
    "            losses['adam0'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                    y_: y_train[start: start + BATCH_SIZE],keep_prob:1})\n",
    "            train_accu['adam0'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x:x_test[0:test_size],y_:y_test[0:test_size],keep_prob:1})\n",
    "            test_accu['adam0'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adam0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T09:53:12.784897Z",
     "start_time": "2018-10-19T09:53:12.736989Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam0_1():\n",
    "    def mycapper4(grads_and_vars):\n",
    "        beta_1=0.9\n",
    "        beta_2=0.9\n",
    "        for i in range(len(grads_and_vars)):\n",
    "            x=tf.sign(grads_and_vars[i][0])\n",
    "            capped_grads_and_vars_momentum_1[i]=(beta_1*capped_grads_and_vars_momentum_1[i][0]+(1-beta_1)*x,grads_and_vars[i][1])\n",
    "            capped_grads_and_vars_momentum_2[i]=(beta_2*capped_grads_and_vars_momentum_2[i][0]+(1-beta_2)*x*x,grads_and_vars[i][1])\n",
    "            capped_grads_and_vars[i]=(tf.sign(capped_grads_and_vars_momentum_1[i][0])/(tf.sqrt(capped_grads_and_vars_momentum_2[i][0])+1e-7),capped_grads_and_vars_momentum_1[i][1])\n",
    "        return capped_grads_and_vars \n",
    "        \n",
    "    opt =tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    grads_and_vars = opt.compute_gradients(cross_entropy,var,gate_gradients=opt.GATE_GRAPH )\n",
    "    capped_grads_and_vars_momentum_1=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars_momentum_2=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=mycapper4(grads_and_vars)  \n",
    "    train_step=opt.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    losses['adam0_1'].clear()\n",
    "    test_accu['adam0_1'].clear()\n",
    "    train_accu['adam0_1'].clear()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            ### batch run\n",
    "            start = i * BATCH_SIZE % train_size\n",
    "            train_step.run(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:0.5})\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:1})\n",
    "            losses['adam0_1'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                    y_: y_train[start: start + BATCH_SIZE],keep_prob:1})\n",
    "            train_accu['adam0_1'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x:x_test[0:test_size],y_:y_test[0:test_size],keep_prob:1})\n",
    "            test_accu['adam0_1'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T09:53:37.740185Z",
     "start_time": "2018-10-19T09:53:15.351046Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0 step\n",
      "train_loss_now : 6.560619\n",
      "train_accu_now : 0.1875\n",
      "test_accu_now : 0.045\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-60f38bd3d009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0madam0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0madam0_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2975a482c357>\u001b[0m in \u001b[0;36madam\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[1;31m### loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             loss_now=sess.run(cross_entropy,{x: x_train[start: start + BATCH_SIZE],\n\u001b[0;32m---> 17\u001b[0;31m                                 y_: y_train[start: start + BATCH_SIZE],keep_prob:1})\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_now\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[1;31m### train_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\胡洋\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adam()\n",
    "adam0()\n",
    "adam0_1()\n",
    "sgd()\n",
    "\n",
    "model_name='alex'\n",
    "data_name='oxflower'\n",
    "\n",
    "def save_me():\n",
    "    loss_name=\"npy/losses batch\"+str(BATCH_SIZE)+'lr'+str(lr)+'iter'+str(epoch)+str(model_name)+str(data_name)+\".npy\"\n",
    "    np.save(loss_name,losses)\n",
    "    \n",
    "    trainaccu_name=\"npy/trainaccu batch\"+str(BATCH_SIZE)+'lr'+str(lr)+'iter'+str(epoch)+str(model_name)+str(data_name)+\".npy\"\n",
    "    np.save(trainaccu_name,train_accu)\n",
    "    \n",
    "    testaccu_name=\"npy/testaccu batch\"+str(BATCH_SIZE)+'lr'+str(lr)+'iter'+str(epoch)+str(model_name)+str(data_name)+\".npy\"\n",
    "    np.save(testaccu_name,test_accu)\n",
    "    \n",
    "save_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(100,5))\n",
    "plt.plot(losses['sgd'], label='SGD')\n",
    "plt.plot(losses['adam'], label='ADAM')\n",
    "plt.plot(losses['adam0'], label='ADAM0')\n",
    "plt.plot(losses['adam0_1'], label='ADAM0_1')\n",
    "\n",
    "plt.title('learning rate = '+str(lr))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,5))\n",
    "plt.plot(train_accu['sgd'], label='SGD')\n",
    "plt.plot(train_accu['adam'], label='ADAM')\n",
    "plt.plot(train_accu['adam0'], label='ADAM0')\n",
    "plt.plot(train_accu['adam0_1'], label='ADAM0_1')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('learning rate = '+str(lr))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"train accuracy\")\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,5))\n",
    "plt.plot(test_accu['sgd'], label='SGD')\n",
    "plt.plot(test_accu['adam'], label='ADAM')\n",
    "plt.plot(test_accu['adam0'], label='ADAM0')\n",
    "plt.plot(test_accu['adam0_1'], label='ADAM0_1')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('learning rate = '+str(lr))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"test accuracy\")\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## af"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T14:15:16.736127Z",
     "start_time": "2018-10-22T14:15:14.352896Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#################################################################\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "fashion = input_data.read_data_sets('data/fashion',one_hot=True)\n",
    "mnist = fashion\n",
    "\n",
    "x=tf.placeholder(tf.float32,[None,784],name='x')\n",
    "y_=tf.placeholder(tf.float32,[None,10],name='y_');\n",
    "\n",
    "x_image=tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "\n",
    "#################################################################\n",
    "keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "# Building 'AlexNet'\n",
    "# network = input_data(shape=[None, 224, 224, 3])\n",
    "network = conv_2d(x_image, 96, 11, strides=4, activation='relu')\n",
    "network = max_pool_2d(network, 2, strides=2)\n",
    "network = local_response_normalization(network)\n",
    "network = conv_2d(network, 256, 5, activation='relu')\n",
    "network = max_pool_2d(network, 2, strides=2)\n",
    "network = local_response_normalization(network)\n",
    "network = conv_2d(network, 384, 3, activation='relu')\n",
    "network = conv_2d(network, 384, 3, activation='relu')\n",
    "network = conv_2d(network, 256, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2, strides=2)\n",
    "network = local_response_normalization(network)\n",
    "network = fully_connected(network, 4096, activation='tanh')\n",
    "network = dropout(network, keep_prob)\n",
    "network = fully_connected(network, 4096, activation='tanh')\n",
    "network = dropout(network, keep_prob)\n",
    "network = fully_connected(network, 10, activation='softmax')\n",
    "\n",
    "#################################################################\n",
    "y=network \n",
    "train_params = tf.trainable_variables()\n",
    "for i in range(len(train_params)):\n",
    "    print(train_params[i].dtype)\n",
    "#################################################################\n",
    "\n",
    "# network = regression(network, optimizer='momentum',\n",
    "#                      loss='categorical_crossentropy',\n",
    "#                      learning_rate=0.001)\n",
    "\n",
    "# # Training\n",
    "# model = tflearn.DNN(network, checkpoint_path='model_alexnet',\n",
    "#                     max_checkpoints=1, tensorboard_verbose=2)\n",
    "# model.fit(X, Y, n_epoch=1000, validation_set=0.1, shuffle=True,\n",
    "#           show_metric=True, batch_size=64, snapshot_step=200,\n",
    "#           snapshot_epoch=False, run_id='alexnet_oxflowers17')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get train params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T14:15:36.223528Z",
     "start_time": "2018-10-22T14:15:36.219539Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var=[]\n",
    "for i in range(len(train_params)):\n",
    "    var.append(train_params[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tune hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T14:15:45.512084Z",
     "start_time": "2018-10-22T14:15:45.507093Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size=1000\n",
    "test_size=200     #can't be too big \n",
    "lr=1e-3           #1e-4 instead\n",
    "BATCH_SIZE = 128   #16,32,64,128\n",
    "epoch = 500       #corrected, same as epcho\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T09:56:48.842771Z",
     "start_time": "2018-10-22T09:56:48.819864Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y+1e-8), reduction_indices = [1]))\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "losses    = {'sgd':[],'adam':[],'adam0':[],'adam0_1':[]} \n",
    "test_accu = {'sgd':[],'adam':[],'adam0':[],'adam0_1':[]}\n",
    "train_accu= {'sgd':[],'adam':[],'adam0':[],'adam0_1':[]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T09:56:51.022941Z",
     "start_time": "2018-10-22T09:56:50.990029Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd():\n",
    "    def mycapper0(gradient):\n",
    "        return gradient \n",
    "    opt =tf.train.GradientDescentOptimizer(learning_rate= lr)\n",
    "    grads_and_vars = opt.compute_gradients(cross_entropy,var,gate_gradients=opt.GATE_GRAPH)\n",
    "    capped_grads_and_vars = [(mycapper0(gv[0]), gv[1]) for gv in grads_and_vars]\n",
    "    train_step=opt.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    losses['sgd'].clear()\n",
    "    test_accu['sgd'].clear()\n",
    "    train_accu['sgd'].clear()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            batch=mnist.train.next_batch(BATCH_SIZE)\n",
    "            train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            losses['sgd'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            train_accu['sgd'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x:mnist.test.images[:2000],y_:mnist.test.labels[:2000],keep_prob:1.})\n",
    "            test_accu['sgd'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam():\n",
    "    train_step=tf.train.AdamOptimizer(learning_rate=lr).minimize(cross_entropy)\n",
    "    \n",
    "    losses['adam'].clear()\n",
    "    test_accu['adam'].clear()\n",
    "    train_accu['adam'].clear()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            batch=mnist.train.next_batch(BATCH_SIZE)\n",
    "            train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            losses['adam'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            train_accu['adam'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x:mnist.test.images[:2000],y_:mnist.test.labels[:2000],keep_prob:1.})\n",
    "            test_accu['adam'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adam0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam0():\n",
    "    def mycapper4(grads_and_vars):\n",
    "        beta_1=0.9\n",
    "        beta_2=0.9\n",
    "        for i in range(len(grads_and_vars)):\n",
    "            capped_grads_and_vars_momentum_1[i]=(beta_1*capped_grads_and_vars[i][0]+(1-beta_1)*tf.sign(grads_and_vars[i][0]),grads_and_vars[i][1])\n",
    "            capped_grads_and_vars_momentum_2[i]=(beta_2*capped_grads_and_vars[i][0]+(1-beta_2),grads_and_vars[i][1])\n",
    "            capped_grads_and_vars[i]=(tf.sign(capped_grads_and_vars_momentum_1[i][0])/(tf.sqrt(capped_grads_and_vars_momentum_2[i][0])+1e-3),capped_grads_and_vars_momentum_1[i][1])\n",
    "        return capped_grads_and_vars   \n",
    "        \n",
    "    opt =tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    grads_and_vars = opt.compute_gradients(cross_entropy,var,gate_gradients=opt.GATE_GRAPH )\n",
    "    capped_grads_and_vars_momentum_1=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars_momentum_2=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=mycapper4(grads_and_vars)  \n",
    "    train_step=opt.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    losses['adam0'].clear()\n",
    "    test_accu['adam0'].clear()\n",
    "    train_accu['adam0'].clear()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            batch=mnist.train.next_batch(BATCH_SIZE)\n",
    "            train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            losses['adam0'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            train_accu['adam0'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x:mnist.test.images[:2000],y_:mnist.test.labels[:2000],keep_prob:1.})\n",
    "            test_accu['adam0'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adam0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam0_1():\n",
    "    def mycapper4(grads_and_vars):\n",
    "        beta_1=0.9\n",
    "        beta_2=0.9\n",
    "        for i in range(len(grads_and_vars)):\n",
    "            x=tf.sign(grads_and_vars[i][0])\n",
    "            capped_grads_and_vars_momentum_1[i]=(beta_1*capped_grads_and_vars_momentum_1[i][0]+(1-beta_1)*x,grads_and_vars[i][1])\n",
    "            capped_grads_and_vars_momentum_2[i]=(beta_2*capped_grads_and_vars_momentum_2[i][0]+(1-beta_2)*x*x,grads_and_vars[i][1])\n",
    "            capped_grads_and_vars[i]=(tf.sign(capped_grads_and_vars_momentum_1[i][0])/(tf.sqrt(capped_grads_and_vars_momentum_2[i][0])+1e-7),capped_grads_and_vars_momentum_1[i][1])\n",
    "        return capped_grads_and_vars \n",
    "        \n",
    "    opt =tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    grads_and_vars = opt.compute_gradients(cross_entropy,var,gate_gradients=opt.GATE_GRAPH )\n",
    "    capped_grads_and_vars_momentum_1=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars_momentum_2=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=mycapper4(grads_and_vars)  \n",
    "    train_step=opt.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    losses['adam0_1'].clear()\n",
    "    test_accu['adam0_1'].clear()\n",
    "    train_accu['adam0_1'].clear()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            batch=mnist.train.next_batch(BATCH_SIZE)\n",
    "            train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            losses['adam0_1'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x:batch[0],y_:batch[1],keep_prob:1.})\n",
    "            train_accu['adam0_1'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x:mnist.test.images[:2000],y_:mnist.test.labels[:2000],keep_prob:1.})\n",
    "            test_accu['adam0_1'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T14:22:19.098738Z",
     "start_time": "2018-10-22T14:22:19.094748Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adam()\n",
    "adam0()\n",
    "adam0_1()\n",
    "sgd()\n",
    "\n",
    "model_name='alex'\n",
    "data_name='fashion'\n",
    "\n",
    "def save_me():\n",
    "    loss_name=\"npy/losses batch\"+str(BATCH_SIZE)+'lr'+str(lr)+'iter'+str(epoch)+str(model_name)+str(data_name)+\".npy\"\n",
    "    np.save(loss_name,losses)\n",
    "    \n",
    "    trainaccu_name=\"npy/trainaccu batch\"+str(BATCH_SIZE)+'lr'+str(lr)+'iter'+str(epoch)+str(model_name)+str(data_name)+\".npy\"\n",
    "    np.save(trainaccu_name,train_accu)\n",
    "    \n",
    "    testaccu_name=\"npy/testaccu batch\"+str(BATCH_SIZE)+'lr'+str(lr)+'iter'+str(epoch)+str(model_name)+str(data_name)+\".npy\"\n",
    "    np.save(testaccu_name,test_accu)\n",
    "    \n",
    "save_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(100,5))\n",
    "plt.plot(losses['sgd'], label='SGD')\n",
    "plt.plot(losses['adam'], label='ADAM')\n",
    "plt.plot(losses['adam0'], label='ADAM0')\n",
    "plt.plot(losses['adam0_1'], label='ADAM0_1')\n",
    "\n",
    "plt.title('learning rate = '+str(lr))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,5))\n",
    "plt.plot(train_accu['sgd'], label='SGD')\n",
    "plt.plot(train_accu['adam'], label='ADAM')\n",
    "plt.plot(train_accu['adam0'], label='ADAM0')\n",
    "plt.plot(train_accu['adam0_1'], label='ADAM0_1')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('learning rate = '+str(lr))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"train accuracy\")\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,5))\n",
    "plt.plot(test_accu['sgd'], label='SGD')\n",
    "plt.plot(test_accu['adam'], label='ADAM')\n",
    "plt.plot(test_accu['adam0'], label='ADAM0')\n",
    "plt.plot(test_accu['adam0_1'], label='ADAM0_1')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('learning rate = '+str(lr))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"test accuracy\")\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T11:50:26.342987Z",
     "start_time": "2018-10-22T11:50:17.376000Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Conv2D/W:0' shape=(11, 11, 3, 96) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D/b:0' shape=(96,) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_1/W:0' shape=(5, 5, 96, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_1/b:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_2/W:0' shape=(3, 3, 256, 384) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_2/b:0' shape=(384,) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_3/W:0' shape=(3, 3, 384, 384) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_3/b:0' shape=(384,) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_4/W:0' shape=(3, 3, 384, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_4/b:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'FullyConnected/W:0' shape=(256, 4096) dtype=float32_ref>,\n",
       " <tf.Variable 'FullyConnected/b:0' shape=(4096,) dtype=float32_ref>,\n",
       " <tf.Variable 'FullyConnected_1/W:0' shape=(4096, 4096) dtype=float32_ref>,\n",
       " <tf.Variable 'FullyConnected_1/b:0' shape=(4096,) dtype=float32_ref>,\n",
       " <tf.Variable 'FullyConnected_2/W:0' shape=(4096, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'FullyConnected_2/b:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "##############################################get cifar\n",
    "train_data = {b'data':[], b'labels':[]} \n",
    "for i in range(5):\n",
    "    with open(\"cifar-10-batches-py/data_batch_\" + str(i + 1), mode='rb') as file:\n",
    "        data = pickle.load(file, encoding='bytes')\n",
    "        train_data[b'data'] += list(data[b'data'])\n",
    "        train_data[b'labels'] += data[b'labels']\n",
    "\n",
    "with open(\"cifar-10-batches-py/test_batch\", mode='rb') as file:\n",
    "    test_data = pickle.load(file, encoding='bytes')\n",
    "    \n",
    "x_train = np.array(train_data[b'data']) / 255\n",
    "y_train = np.array(pd.get_dummies(train_data[b'labels']))\n",
    "x_test = test_data[b'data'] / 255\n",
    "y_test = np.array(pd.get_dummies(test_data[b'labels']))\n",
    "##############################################get cifar\n",
    "\n",
    "    \n",
    "##############################################handle cifar\n",
    "x = tf.placeholder(tf.float32, [None, 3072],name='x')\n",
    "y_ = tf.placeholder(tf.float32, [None, 10],name='y_')\n",
    "x_image = tf.reshape(x, [-1, 3, 32, 32])\n",
    "x_image = tf.transpose(x_image, [0, 2, 3, 1])\n",
    "##############################################handle cifar\n",
    "\n",
    "\n",
    "##############################################model construct\n",
    "keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "# Building 'AlexNet'\n",
    "# network = input_data(shape=[None, 224, 224, 3])\n",
    "network = conv_2d(x_image, 96, 11, strides=4, activation='relu')\n",
    "network = max_pool_2d(network, 3, strides=2)\n",
    "network = local_response_normalization(network)\n",
    "network = conv_2d(network, 256, 5, activation='relu')\n",
    "network = max_pool_2d(network, 3, strides=2)\n",
    "network = local_response_normalization(network)\n",
    "network = conv_2d(network, 384, 3, activation='relu')\n",
    "network = conv_2d(network, 384, 3, activation='relu')\n",
    "network = conv_2d(network, 256, 3, activation='relu')\n",
    "network = max_pool_2d(network, 3, strides=2)\n",
    "network = local_response_normalization(network)\n",
    "network = fully_connected(network, 4096, activation='tanh')\n",
    "network = dropout(network, keep_prob)\n",
    "network = fully_connected(network, 4096, activation='tanh')\n",
    "network = dropout(network, keep_prob)\n",
    "network = fully_connected(network, 10, activation='softmax')\n",
    "\n",
    "\n",
    "##############################################model construct\n",
    "\n",
    "\n",
    "y = network\n",
    "\n",
    "# train_params = tflearn.variables.get_all_variables ()\n",
    "\n",
    "train_params=tf.trainable_variables()\n",
    "train_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get train params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T12:09:36.725166Z",
     "start_time": "2018-10-22T12:09:36.716191Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Conv2D/W:0' shape=(11, 11, 3, 96) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D/b:0' shape=(96,) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_1/W:0' shape=(5, 5, 96, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_1/b:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_2/W:0' shape=(3, 3, 256, 384) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_2/b:0' shape=(384,) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_3/W:0' shape=(3, 3, 384, 384) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_3/b:0' shape=(384,) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_4/W:0' shape=(3, 3, 384, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'Conv2D_4/b:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'FullyConnected/W:0' shape=(256, 4096) dtype=float32_ref>,\n",
       " <tf.Variable 'FullyConnected/b:0' shape=(4096,) dtype=float32_ref>,\n",
       " <tf.Variable 'FullyConnected_1/W:0' shape=(4096, 4096) dtype=float32_ref>,\n",
       " <tf.Variable 'FullyConnected_1/b:0' shape=(4096,) dtype=float32_ref>,\n",
       " <tf.Variable 'FullyConnected_2/W:0' shape=(4096, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'FullyConnected_2/b:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=[]\n",
    "for i in range(len(train_params)):\n",
    "    if train_params[i].dtype=='float32_ref':\n",
    "        var.append(train_params[i])\n",
    "        \n",
    "var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tune hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T12:23:02.622511Z",
     "start_time": "2018-10-22T12:23:02.617525Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "lr = 1e-4\n",
    "train_size = len(x_train)\n",
    "test_size=100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T12:23:11.969526Z",
     "start_time": "2018-10-22T12:23:11.851993Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices = [1]))\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "losses    = {'sgd':[],'adam':[],'adam0':[],'adam0_1':[]} \n",
    "test_accu = {'sgd':[],'adam':[],'adam0':[],'adam0_1':[]}\n",
    "train_accu= {'sgd':[],'adam':[],'adam0':[],'adam0_1':[]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T12:29:50.628416Z",
     "start_time": "2018-10-22T12:29:50.456874Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd():\n",
    "    def mycapper0(gradient):\n",
    "        return gradient \n",
    "    opt =tf.train.GradientDescentOptimizer(learning_rate= lr)\n",
    "    grads_and_vars = opt.compute_gradients(cross_entropy,var,gate_gradients=opt.GATE_GRAPH)\n",
    "    capped_grads_and_vars = [(mycapper0(gv[0]), gv[1]) for gv in grads_and_vars]\n",
    "    train_step=opt.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    losses['sgd'].clear()\n",
    "    test_accu['sgd'].clear()\n",
    "    train_accu['sgd'].clear()\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            start = i * BATCH_SIZE % train_size\n",
    "            train_step.run(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:0.5})\n",
    "\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:1.})\n",
    "            losses['sgd'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:1.})\n",
    "            train_accu['sgd'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x: x_test[0: test_size],\n",
    "                                y_: y_test[0:test_size],keep_prob:1.})\n",
    "            test_accu['sgd'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam():\n",
    "    train_step=tf.train.AdamOptimizer(learning_rate=lr).minimize(cross_entropy)\n",
    "\n",
    "    \n",
    "    losses['adam'].clear()\n",
    "    test_accu['adam'].clear()\n",
    "    train_accu['adam'].clear()\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            start = i * BATCH_SIZE % train_size\n",
    "            train_step.run(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:0.5})\n",
    "\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:1.})\n",
    "            losses['adam'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:1.})\n",
    "            train_accu['adam'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x: x_test[0: test_size],\n",
    "                                y_: y_test[0:test_size],keep_prob:1.})\n",
    "            test_accu['adam'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adam0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam0():\n",
    "    def mycapper4(grads_and_vars):\n",
    "        beta_1=0.9\n",
    "        beta_2=0.9\n",
    "        for i in range(len(grads_and_vars)):\n",
    "            capped_grads_and_vars_momentum_1[i]=(beta_1*capped_grads_and_vars[i][0]+(1-beta_1)*tf.sign(grads_and_vars[i][0]),grads_and_vars[i][1])\n",
    "            capped_grads_and_vars_momentum_2[i]=(beta_2*capped_grads_and_vars[i][0]+(1-beta_2),grads_and_vars[i][1])\n",
    "            capped_grads_and_vars[i]=(tf.sign(capped_grads_and_vars_momentum_1[i][0])/(tf.sqrt(capped_grads_and_vars_momentum_2[i][0])+1e-3),capped_grads_and_vars_momentum_1[i][1])\n",
    "        return capped_grads_and_vars   \n",
    "        \n",
    "    opt =tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    grads_and_vars = opt.compute_gradients(cross_entropy,var,gate_gradients=opt.GATE_GRAPH )\n",
    "    capped_grads_and_vars_momentum_1=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars_momentum_2=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=mycapper4(grads_and_vars)  \n",
    "    train_step=opt.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    losses['adam0'].clear()\n",
    "    test_accu['adam0'].clear()\n",
    "    train_accu['adam0'].clear()\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            start = i * BATCH_SIZE % train_size\n",
    "            train_step.run(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:0.5})\n",
    "\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:1.})\n",
    "            losses['adam0'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:1.})\n",
    "            train_accu['adam0'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x: x_test[0: test_size],\n",
    "                                y_: y_test[0:test_size],keep_prob:1.})\n",
    "            test_accu['adam0'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adam0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam0_1():\n",
    "    def mycapper4(grads_and_vars):\n",
    "        beta_1=0.9\n",
    "        beta_2=0.9\n",
    "        for i in range(len(grads_and_vars)):\n",
    "            x=tf.sign(grads_and_vars[i][0])\n",
    "            capped_grads_and_vars_momentum_1[i]=(beta_1*capped_grads_and_vars_momentum_1[i][0]+(1-beta_1)*x,grads_and_vars[i][1])\n",
    "            capped_grads_and_vars_momentum_2[i]=(beta_2*capped_grads_and_vars_momentum_2[i][0]+(1-beta_2)*x*x,grads_and_vars[i][1])\n",
    "            capped_grads_and_vars[i]=(tf.sign(capped_grads_and_vars_momentum_1[i][0])/(tf.sqrt(capped_grads_and_vars_momentum_2[i][0])+1e-7),capped_grads_and_vars_momentum_1[i][1])\n",
    "        return capped_grads_and_vars \n",
    "        \n",
    "    opt =tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    grads_and_vars = opt.compute_gradients(cross_entropy,var,gate_gradients=opt.GATE_GRAPH )\n",
    "    capped_grads_and_vars_momentum_1=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars_momentum_2=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=[(0,0)  for x in range(len(grads_and_vars))]\n",
    "    capped_grads_and_vars=mycapper4(grads_and_vars)  \n",
    "    train_step=opt.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    losses['adam0_1'].clear()\n",
    "    test_accu['adam0_1'].clear()\n",
    "    train_accu['adam0_1'].clear()\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(epoch):\n",
    "            start = i * BATCH_SIZE % train_size\n",
    "            train_step.run(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:0.5})\n",
    "\n",
    "            ### loss \n",
    "            loss_now=sess.run(cross_entropy,{x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:1.})\n",
    "            losses['adam0_1'].append(loss_now)\n",
    "            ### train_acc\n",
    "            train_accu_now=accuracy.eval(feed_dict={x: x_train[start: start + BATCH_SIZE],\n",
    "                                y_: y_train[start: start + BATCH_SIZE],keep_prob:1.})\n",
    "            train_accu['adam0_1'].append(train_accu_now)\n",
    "            ### test_accu\n",
    "            test_accu_now=accuracy.eval(feed_dict={x: x_test[0: test_size],\n",
    "                                y_: y_test[0:test_size],keep_prob:1.})\n",
    "            test_accu['adam0_1'].append(test_accu_now)\n",
    "            ### print things\n",
    "            print(str(i)+' step')\n",
    "            print('train_loss_now :',loss_now)\n",
    "            print('train_accu_now :',train_accu_now)\n",
    "            print('test_accu_now :',test_accu_now)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam()\n",
    "adam0()\n",
    "adam0_1()\n",
    "sgd()\n",
    "\n",
    "model_name='alex'\n",
    "data_name='cifar'\n",
    "\n",
    "def save_me():\n",
    "    loss_name=\"npy/losses batch\"+str(BATCH_SIZE)+'lr'+str(lr)+'iter'+str(epoch)+str(model_name)+str(data_name)+\".npy\"\n",
    "    np.save(loss_name,losses)\n",
    "    \n",
    "    trainaccu_name=\"npy/trainaccu batch\"+str(BATCH_SIZE)+'lr'+str(lr)+'iter'+str(epoch)+str(model_name)+str(data_name)+\".npy\"\n",
    "    np.save(trainaccu_name,train_accu)\n",
    "    \n",
    "    testaccu_name=\"npy/testaccu batch\"+str(BATCH_SIZE)+'lr'+str(lr)+'iter'+str(epoch)+str(model_name)+str(data_name)+\".npy\"\n",
    "    np.save(testaccu_name,test_accu)\n",
    "    \n",
    "save_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(100,5))\n",
    "plt.plot(losses['sgd'], label='SGD')\n",
    "plt.plot(losses['adam'], label='ADAM')\n",
    "plt.plot(losses['adam0'], label='ADAM0')\n",
    "plt.plot(losses['adam0_1'], label='ADAM0_1')\n",
    "\n",
    "plt.title('learning rate = '+str(lr))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trian accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,5))\n",
    "plt.plot(train_accu['sgd'], label='SGD')\n",
    "plt.plot(train_accu['adam'], label='ADAM')\n",
    "plt.plot(train_accu['adam0'], label='ADAM0')\n",
    "plt.plot(train_accu['adam0_1'], label='ADAM0_1')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('learning rate = '+str(lr))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"train accuracy\")\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,5))\n",
    "plt.plot(test_accu['sgd'], label='SGD')\n",
    "plt.plot(test_accu['adam'], label='ADAM')\n",
    "plt.plot(test_accu['adam0'], label='ADAM0')\n",
    "plt.plot(test_accu['adam0_1'], label='ADAM0_1')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('learning rate = '+str(lr))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"test accuracy\")\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## at（coming soon。。。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T14:23:59.867286Z",
     "start_time": "2018-10-22T14:23:59.863296Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### waiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "ENDLESS",
   "title_sidebar": "FUCK_YOU",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "30",
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 416.518518,
   "position": {
    "height": "40px",
    "left": "1050.23px",
    "right": "20px",
    "top": "119.988px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
